{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_tensors as lt\n",
    "\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "import torch.distributions as D\n",
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "import torchvision.transforms as TF\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # if torch.backends.mps.is_available():\n",
    "    # return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "DEVICE = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\").to(DEVICE)\n",
    "# vae = cast(AutoencoderKL, torch.compile(vae, mode=\"max-autotune\"))\n",
    "vae.eval()\n",
    "if DEVICE == torch.device(\"cuda\"):\n",
    "    # vae = cast(AutoencoderKL, torch.compile(vae, mode=\"max-autotune\"))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_processor = VaeImageProcessor(do_normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TVF\n",
    "\n",
    "def normalize(x: torch.Tensor) -> torch.Tensor:\n",
    "    return cast(torch.Tensor, vae_processor.normalize(x))\n",
    "\n",
    "\n",
    "def denormalize(x: torch.Tensor) -> torch.Tensor:\n",
    "    return cast(torch.Tensor, vae_processor.denormalize(x))\n",
    "\n",
    "\n",
    "def resize(x: torch.Tensor) -> torch.Tensor:\n",
    "    return TVF.resize(x, [256, 256])\n",
    "\n",
    "transform = TF.Compose(\n",
    "    [\n",
    "        TF.ToTensor(),\n",
    "        TF.RandomHorizontalFlip(),\n",
    "        TF.Lambda(lambda x: normalize(x)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train: bool = True):\n",
    "        self.cifar = torchvision.datasets.CIFAR10(\n",
    "            root=\"/tmp/cifar\", download=True, train=train, transform=transform\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x, y = self.cifar[idx]\n",
    "        t = torch.rand(1)\n",
    "        return x, y, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train = CifarDataset(train=True)\n",
    "cifar_test = CifarDataset(train=False)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    cifar_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(cifar_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "x = resize(next(iter(train_dataloader))[0].to(DEVICE))\n",
    "latent_shape = (\n",
    "    vae.encode(x, return_dict=False)[0]\n",
    "    .mean[0]\n",
    "    .shape\n",
    ")\n",
    "\n",
    "(\n",
    "    next(iter(train_dataloader))[0],\n",
    "    next(iter(train_dataloader))[1],\n",
    "    next(iter(train_dataloader))[2],\n",
    "    latent_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import DiT\n",
    "\n",
    "model = DiT(\n",
    "    d_model=768,\n",
    "    patch_size=4,\n",
    "    img_size=(32, 32),\n",
    "    n_heads=12,\n",
    "    n_layers=28,\n",
    "    in_channels=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 200\n",
    "\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=N_EPOCHS)\n",
    "\n",
    "ckpt_path = f\"../ckpt/{DiT.__name__}\"\n",
    "if ckpt_path and False:\n",
    "    model.load_state_dict(torch.load(f\"{ckpt_path}/model.pth\", map_location=DEVICE))\n",
    "    optimizer.load_state_dict(\n",
    "        torch.load(f\"{ckpt_path}/optimizer.pth\", map_location=DEVICE)\n",
    "    )\n",
    "    scheduler.load_state_dict(\n",
    "        torch.load(f\"{ckpt_path}/scheduler.pth\", map_location=DEVICE)\n",
    "    )\n",
    "\n",
    "if DEVICE == torch.device(\"cuda\"):\n",
    "    pass\n",
    "    # model = cast(DiT, torch.compile(model, mode=\"max-autotune\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Param count: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_with_ode(\n",
    "    model: nn.Module,\n",
    "    n_samples: int = 500,\n",
    "    n_steps: int = 25,\n",
    "):\n",
    "    model.eval()\n",
    "    model_device = next(model.parameters()).device\n",
    "    initial_samples = torch.randn((n_samples, *latent_shape), device=model_device)\n",
    "    t_span = torch.linspace(0.0, 1.0, n_steps).to(model_device)\n",
    "    trajectory = odeint(\n",
    "        model.ode_forward,\n",
    "        initial_samples,\n",
    "        t_span,\n",
    "        method=\"euler\",\n",
    "        atol=1e-5,\n",
    "        rtol=1e-5,\n",
    "    )\n",
    "    trajectory = cast(torch.Tensor, trajectory)\n",
    "\n",
    "    return trajectory[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vae.decode(vae.encode(next(iter(train_dataloader))[0].cuda()[:1]).latent_dist.mean)\n",
    "print(x)\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.transforms.functional import resize\n",
    "import os\n",
    "\n",
    "\n",
    "def to_latent(vae: AutoencoderKL, x_raw: torch.Tensor) -> torch.Tensor:\n",
    "    x_raw = resize(x_raw, [256, 256])\n",
    "    return vae.encode(x_raw, return_dict=False)[0].mean\n",
    "\n",
    "\n",
    "def from_latent(vae: AutoencoderKL, z: torch.Tensor) -> torch.Tensor:\n",
    "    return vae.decode(cast(torch.FloatTensor, z), return_dict=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = denormalize(from_latent(vae, to_latent(vae, next(iter(train_dataloader))[0].to(DEVICE)[:1])))\n",
    "print(x)\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    n_epochs: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    verbose: bool = False,\n",
    "    contrastive_flow_weight: float = 0.00,\n",
    "):\n",
    "    def step(x_raw, t):\n",
    "        x = to_latent(vae, x_raw)\n",
    "        noise = torch.randn_like(x, device=DEVICE)\n",
    "        t_expanded = t.unsqueeze(-1).unsqueeze(-1)\n",
    "        z = t_expanded * x + (1 - t_expanded) * noise\n",
    "        target_u = x - noise\n",
    "        u = model(z, t.squeeze(-1))\n",
    "        loss = F.mse_loss(u, target_u)\n",
    "        if contrastive_flow_weight > 0.0:\n",
    "            u_hat = torch.roll(u, shifts=1, dims=0)\n",
    "            loss_contrastive = F.mse_loss(u, u_hat)\n",
    "            loss = loss - contrastive_flow_weight * loss_contrastive\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_fid(nfe: int = 25):\n",
    "        for i, (val_imgs, _, _) in enumerate(val_dataloader):\n",
    "            if i * BATCH_SIZE > 512:\n",
    "                break\n",
    "            real_images = val_imgs\n",
    "            real_images = denormalize(real_images).clamp(0, 1)\n",
    "            real_images = (real_images * 255).to(torch.uint8).to(DEVICE)  # BS x 3 x 32 x 32\n",
    "            real_images = resize(real_images, [256, 256])\n",
    "            z_s = sample_with_ode(model, n_samples=real_images.shape[0], n_steps=nfe)\n",
    "            generated_images = from_latent(vae, z_s)\n",
    "            generated_images = denormalize(generated_images).clamp(0, 1)\n",
    "            generated_images = (\n",
    "                (generated_images * 255).to(torch.uint8).to(DEVICE)\n",
    "            )  # BS x 3 x 32 x 32\n",
    "            fid = FrechetInceptionDistance().to(DEVICE)\n",
    "            fid.update(real_images, real=True)\n",
    "            fid.update(generated_images, real=False)\n",
    "        return fid.compute()\n",
    "\n",
    "    def ckpt_callback(epoch: int, val_loss: float):\n",
    "        ckpt_dir = \"../ckpt\"\n",
    "        subdir = f\"{ckpt_dir}/{DiT.__name__}\"\n",
    "        if not os.path.exists(subdir):\n",
    "            os.makedirs(subdir)\n",
    "        torch.save(model.state_dict(), f\"{subdir}/model.pth\")\n",
    "        torch.save(optimizer.state_dict(), f\"{subdir}/optimizer.pth\")\n",
    "        torch.save(scheduler.state_dict(), f\"{subdir}/scheduler.pth\")\n",
    "        print(f\"Saved checkpoint at epoch {epoch} with val loss {val_loss:.4f}\")\n",
    "\n",
    "    log_interval = 1\n",
    "    model.train()\n",
    "    best_val_loss = float(\"inf\")\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "    fid_2_history = []\n",
    "    fid_25_history = []\n",
    "    fid_50_history = []\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            losses = []\n",
    "            for x_raw, _, t in tqdm(dataloader):\n",
    "                loss = step(x_raw.to(DEVICE), t.to(DEVICE))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for x_raw, _, t in val_dataloader:\n",
    "                    loss = step(x_raw.to(DEVICE), t.to(DEVICE))\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            current_loss = float(np.mean(losses))\n",
    "            current_val_loss = float(np.mean(val_losses))\n",
    "\n",
    "            fid_2 = get_fid(nfe=2).cpu().numpy()\n",
    "            fid_25 = get_fid(nfe=25).cpu().numpy()\n",
    "            fid_50 = get_fid(nfe=50).cpu().numpy()\n",
    "            fid_2_history.append(fid_2)\n",
    "            fid_25_history.append(fid_25)\n",
    "            fid_50_history.append(fid_50)\n",
    "            if (epoch % log_interval == 0 or epoch == n_epochs - 1) and verbose:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}\\t loss: {current_loss:.4f}\\t val loss: {current_val_loss:.4f}\\t FID_2: {fid_2:.4f}\\t FID_25: {fid_25:.4f}\\t FID_50: {fid_50:.4f}\"\n",
    "                )\n",
    "\n",
    "            loss_history.append(current_loss)\n",
    "            val_loss_history.append(current_val_loss)\n",
    "\n",
    "            if current_val_loss < best_val_loss:\n",
    "                best_val_loss = current_val_loss\n",
    "                ckpt_callback(epoch, current_val_loss)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KeyboardInterrupt\")\n",
    "\n",
    "    train_dict = {\n",
    "        \"loss_history\": loss_history,\n",
    "        \"val_loss_history\": val_loss_history,\n",
    "        \"fid_2_history\": fid_2_history,\n",
    "        \"fid_25_history\": fid_25_history,\n",
    "        \"fid_50_history\": fid_50_history,\n",
    "    }\n",
    "    return train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = train(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    val_dataloader=test_dataloader,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_dict[\"loss_history\"])\n",
    "plt.plot(train_dict[\"val_loss_history\"])\n",
    "plt.legend([\"train\", \"val\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_dict[\"fid_2_history\"])\n",
    "plt.plot(train_dict[\"fid_25_history\"])\n",
    "plt.plot(train_dict[\"fid_50_history\"])\n",
    "plt.legend([\"FID_2\", \"FID_25\", \"FID_50\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_s = sample_with_ode(model, n_samples=4, n_steps=25)\n",
    "generated_images = from_latent(vae, z_s)\n",
    "generated_images = (\n",
    "    (generated_images * 255).to(torch.uint8).to(DEVICE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as TF\n",
    "\n",
    "\n",
    "TF.ToPILImage()(generated_images[2]).resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.ToPILImage()(next(iter(test_dataloader))[0][0]).resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_2 = torch.device(\"cuda:0\")\n",
    "\n",
    "vae = vae.to(device_2)\n",
    "\n",
    "fid = FrechetInceptionDistance().to(device_2)\n",
    "\n",
    "for i, (real, _, _) in enumerate(torch.utils.data.DataLoader(cifar_test, batch_size=4, shuffle=True)):\n",
    "    if i * 4 > 512:\n",
    "        break\n",
    "    real_images = real.to(device_2)\n",
    "\n",
    "    real_images = resize(real_images)\n",
    "    generated_images = resize(generated_images)\n",
    "    generated_images = vae.decode(vae.encode(real_images).latent_dist.mean).sample\n",
    "\n",
    "    real_images = denormalize(real_images)\n",
    "    generated_images = denormalize(generated_images)\n",
    "\n",
    "    real_images = (real_images * 255).to(torch.uint8).to(device_2)\n",
    "    generated_images = (generated_images * 255).to(torch.uint8).to(device_2)\n",
    "\n",
    "\n",
    "    fid.update(real_images, real=True)\n",
    "    fid.update(generated_images, real=False)\n",
    "\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
